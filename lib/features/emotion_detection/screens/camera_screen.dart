import 'dart:io';
import 'dart:math';
import 'dart:typed_data';

import 'package:camera/camera.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';
import 'package:go_router/go_router.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import '../models/hand_landmark_result.dart';
import '../services/bisindo_feature_extractor.dart';
import '../services/hand_landmark_service.dart';
import 'package:image/image.dart' as img;
import 'package:path_provider/path_provider.dart';

import '../../../core/constants/app_colors.dart';
import '../../../core/constants/app_fonts.dart';
import '../../../core/constants/asset_paths.dart';
import '../../../core/routes/app_routes.dart';

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Top-level data classes & functions for compute() isolate
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/// Plane metadata transported to the isolate.
class _PlaneData {
  final Uint8List bytes;
  final int bytesPerRow;
  final int bytesPerPixel;

  _PlaneData({required this.bytes, required this.bytesPerRow, required this.bytesPerPixel});
}

/// Parameters passed into the background isolate for face cropping & resizing.
class _CropParams {
  final int imageWidth;
  final int imageHeight;
  final List<_PlaneData> planes;
  final int bboxLeft;
  final int bboxTop;
  final int bboxWidth;
  final int bboxHeight;

  /// `true` when camera format is BGRA (iOS). `false` for YUV420 (Android).
  final bool isBGRA;

  /// Sensor orientation in degrees (0, 90, 180, 270).
  final int sensorOrientation;

  _CropParams({
    required this.imageWidth,
    required this.imageHeight,
    required this.planes,
    required this.bboxLeft,
    required this.bboxTop,
    required this.bboxWidth,
    required this.bboxHeight,
    required this.isBGRA,
    required this.sensorOrientation,
  });
}

/// Runs in a separate isolate via `compute()`.
///
/// Pipeline (matches training):
///   1. Camera bytes â†’ RGB image  (YUV420 on Android, BGRA on iOS)
///   2. Rotate to upright          (sensorOrientation â†’ portrait)
///   3. Mirror horizontally        (cv2.flip(frame,1) equivalent)
///   4. Transform bbox to mirrored space
///   5. Crop face with 10 % padding
///   6. Resize to 224 Ã— 224
///   7. Encode as JPEG (RGB)
///
/// Returns JPEG-encoded bytes of the processed face (or `null`).
Uint8List? _cropAndResizeFace(_CropParams p) {
  try {
    img.Image? fullImage;

    if (p.isBGRA) {
      // â”€â”€ iOS: BGRA8888 â†’ RGB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      fullImage = img.Image(width: p.imageWidth, height: p.imageHeight);
      final bytes = p.planes[0].bytes;
      final bpr = p.planes[0].bytesPerRow;
      for (int y = 0; y < p.imageHeight; y++) {
        for (int x = 0; x < p.imageWidth; x++) {
          final idx = y * bpr + x * 4;
          if (idx + 3 < bytes.length) {
            fullImage.setPixelRgba(
              x,
              y,
              bytes[idx + 2],
              bytes[idx + 1],
              bytes[idx],
              bytes[idx + 3],
            );
          }
        }
      }
    } else {
      // â”€â”€ Android: YUV420 â†’ RGB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      fullImage = _convertYUV420ToImage(p);
    }

    if (fullImage == null) return null;

    // â”€â”€ Step 1: Rotate to upright (match ML Kit bbox coordinate space) â”€
    // ML Kit applies rotation metadata internally, producing bbox values
    // in the rotated (upright) coordinate space.  We must rotate our raw
    // RGB buffer to the same orientation before cropping.
    if (p.sensorOrientation != 0) {
      fullImage = img.copyRotate(fullImage, angle: p.sensorOrientation);
    }

    // â”€â”€ Step 2: Mirror horizontally â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    // Training used cv2.flip(frame, 1).  We replicate that so the
    // cropped face matches what EfficientNetB2 was trained on.
    img.flipHorizontal(fullImage);

    // â”€â”€ Step 3: Transform bbox from pre-mirror â†’ mirrored space â”€â”€â”€â”€â”€
    // ML Kit bbox is in the rotated-but-NOT-mirrored coordinate space.
    // After horizontal flip:  new_left = width âˆ’ old_left âˆ’ old_width.
    final mirroredBboxLeft = fullImage.width - p.bboxLeft - p.bboxWidth;

    // â”€â”€ Step 4: Crop with 10 % padding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final padX = (p.bboxWidth * 0.10).round();
    final padY = (p.bboxHeight * 0.10).round();

    final cropLeft = (mirroredBboxLeft - padX).clamp(0, fullImage.width - 1);
    final cropTop = (p.bboxTop - padY).clamp(0, fullImage.height - 1);
    final cropWidth = (p.bboxWidth + padX * 2).clamp(1, fullImage.width - cropLeft);
    final cropHeight = (p.bboxHeight + padY * 2).clamp(1, fullImage.height - cropTop);

    final cropped = img.copyCrop(
      fullImage,
      x: cropLeft,
      y: cropTop,
      width: cropWidth,
      height: cropHeight,
    );

    // â”€â”€ Step 5: Resize to 224 Ã— 224 (EfficientNetB2 input) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final resized = img.copyResize(
      cropped,
      width: 224,
      height: 224,
      interpolation: img.Interpolation.linear,
    );

    return Uint8List.fromList(img.encodeJpg(resized, quality: 90));
  } catch (_) {
    return null;
  }
}

/// Converts YUV420 (Android camera) image data to an [img.Image].
img.Image? _convertYUV420ToImage(_CropParams p) {
  try {
    final w = p.imageWidth;
    final h = p.imageHeight;
    final yPlane = p.planes[0].bytes;
    final uPlane = p.planes[1].bytes;
    final vPlane = p.planes[2].bytes;
    final yRowStride = p.planes[0].bytesPerRow;
    final uvRowStride = p.planes[1].bytesPerRow;
    final uvPixelStride = p.planes[1].bytesPerPixel;

    final image = img.Image(width: w, height: h);

    for (int y = 0; y < h; y++) {
      for (int x = 0; x < w; x++) {
        final yIdx = y * yRowStride + x;
        final uvIdx = (y ~/ 2) * uvRowStride + (x ~/ 2) * uvPixelStride;
        if (yIdx >= yPlane.length || uvIdx >= uPlane.length || uvIdx >= vPlane.length) {
          continue;
        }
        final yVal = yPlane[yIdx];
        final uVal = uPlane[uvIdx];
        final vVal = vPlane[uvIdx];

        int r = (yVal + 1.370705 * (vVal - 128)).round().clamp(0, 255);
        int g = (yVal - 0.337633 * (uVal - 128) - 0.698001 * (vVal - 128)).round().clamp(0, 255);
        int b = (yVal + 1.732446 * (uVal - 128)).round().clamp(0, 255);

        image.setPixelRgba(x, y, r, g, b, 255);
      }
    }
    return image;
  } catch (_) {
    return null;
  }
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Camera Screen Widget
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/// Camera Screen â€“ Real-time face detection for emotion analysis.
class CameraScreen extends StatefulWidget {
  const CameraScreen({super.key});

  @override
  State<CameraScreen> createState() => _CameraScreenState();
}

class _CameraScreenState extends State<CameraScreen> {
  // â”€â”€ Camera â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  CameraController? _cameraController;
  List<CameraDescription>? _cameras;
  bool _isCameraInitialized = false;

  // â”€â”€ ML Kit Face Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  late final FaceDetector _faceDetector;
  bool _isProcessingFrame = false;

  // â”€â”€ Throttle control â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  DateTime _lastProcessedTime = DateTime.now();
  static const Duration _throttleInterval = Duration(milliseconds: 100);

  // â”€â”€ Processed face data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Uint8List? _processedFaceBytes;
  File? _processedFaceFile;

  // â”€â”€ UI state â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  bool _isSending = false;
  bool _faceDetected = false;
  Rect? _faceBoundingBox;
  int _imageWidth = 0;
  int _imageHeight = 0;
  int _sensorOrientation = 0;

  // â”€â”€ Hand Landmark / BISINDO Sign Language â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  HandLandmarkService? _handLandmarkService;
  final BisindoFeatureExtractor _featureExtractor = BisindoFeatureExtractor();
  List<List<double>> _motionSequence = [];
  bool _isMotionDataReady = false;

  /// Minimum frames needed before the user CAN send (UX guard).
  static const int _minFramesToSend = 10;

  /// Frames we aim to collect for best quality (shown in UI).
  static const int _targetFrames = 30;

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  //  Lifecycle
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  @override
  void initState() {
    super.initState();
    _initFaceDetector();
    _initHandLandmarkService();
    _initializeCamera();
  }

  @override
  void dispose() {
    _cameraController?.dispose();
    _faceDetector.close();
    _handLandmarkService?.dispose();
    super.dispose();
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  //  Face Detector Initialization
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  void _initFaceDetector() {
    final options = FaceDetectorOptions(
      enableClassification: false,
      enableLandmarks: false,
      enableContours: false,
      enableTracking: false,
      performanceMode: FaceDetectorMode.fast,
      minFaceSize: 0.15,
    );
    _faceDetector = FaceDetector(options: options);
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  //  Hand Landmark Service Initialization (BISINDO Sign Language)
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  void _initHandLandmarkService() {
    try {
      _handLandmarkService = HandLandmarkService.create();
      debugPrint('[CameraScreen] âœ… Hand landmark service initialized');
    } catch (e) {
      debugPrint('[CameraScreen] âš ï¸ Hand landmark init failed: $e');
    }
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  //  Camera Initialization + Image Stream
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Future<void> _initializeCamera() async {
    try {
      _cameras = await availableCameras();
      if (_cameras == null || _cameras!.isEmpty) return;

      final frontCamera = _cameras!.firstWhere(
        (c) => c.lensDirection == CameraLensDirection.front,
        orElse: () => _cameras!.first,
      );

      _cameraController = CameraController(
        frontCamera,
        ResolutionPreset.medium,
        enableAudio: false,
        imageFormatGroup: Platform.isAndroid ? ImageFormatGroup.yuv420 : ImageFormatGroup.bgra8888,
      );

      await _cameraController!.initialize();
      if (!mounted) return;

      setState(() => _isCameraInitialized = true);

      // Start the image stream for real-time face detection
      _cameraController!.startImageStream(_onCameraFrame);
    } catch (e) {
      debugPrint('[CameraScreen] Error initializing camera: $e');
    }
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  //  Frame Processing (throttled)
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  void _onCameraFrame(CameraImage cameraImage) {
    if (_isProcessingFrame) return;
    final now = DateTime.now();
    if (now.difference(_lastProcessedTime) < _throttleInterval) return;

    _isProcessingFrame = true;
    _lastProcessedTime = now;
    _processFrame(cameraImage);
  }

  Future<void> _processFrame(CameraImage cameraImage) async {
    try {
      // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      // Run hand detection FIRST (synchronous via JNI) while the
      // CameraImage buffer is still valid.
      // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

      // 0. Hand Landmark Detection (synchronous â€” must run before any await)
      ({HandLandmarkResult? left, HandLandmarkResult? right})? handResults;
      if (_handLandmarkService != null && _handLandmarkService!.isInitialized) {
        try {
          final sensorOrientation = _cameraController!.description.sensorOrientation;
          handResults = _handLandmarkService!.detectHands(cameraImage, sensorOrientation);
        } catch (e) {
          debugPrint('[CameraScreen] Hand landmark error: $e');
        }
      }

      // 1. Build InputImage for ML Kit (copies bytes synchronously)
      final inputImage = _buildInputImage(cameraImage);
      if (inputImage == null) {
        _isProcessingFrame = false;
        return;
      }

      // â”€â”€ 2. Face Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      final faces = await _faceDetector.processImage(inputImage);

      if (faces.isEmpty) {
        if (mounted) {
          setState(() {
            _faceDetected = false;
            _faceBoundingBox = null;
          });
        }
      } else {
        final face = faces.first;
        final bbox = face.boundingBox;

        debugPrint(
          '[CameraScreen] âœ… Wajah terdeteksi! '
          'BBox: left=${bbox.left.toInt()}, top=${bbox.top.toInt()}, '
          'w=${bbox.width.toInt()}, h=${bbox.height.toInt()}',
        );

        if (mounted) {
          setState(() {
            _faceDetected = true;
            _faceBoundingBox = bbox;
            _imageWidth = cameraImage.width;
            _imageHeight = cameraImage.height;
            _sensorOrientation = _cameraController!.description.sensorOrientation;
          });
        }

        // 3. Crop & resize face in a background isolate
        final planes = cameraImage.planes
            .map(
              (p) => _PlaneData(
                bytes: Uint8List.fromList(p.bytes),
                bytesPerRow: p.bytesPerRow,
                bytesPerPixel: p.bytesPerPixel ?? 1,
              ),
            )
            .toList();

        final cropParams = _CropParams(
          imageWidth: cameraImage.width,
          imageHeight: cameraImage.height,
          planes: planes,
          bboxLeft: bbox.left.toInt(),
          bboxTop: bbox.top.toInt(),
          bboxWidth: bbox.width.toInt(),
          bboxHeight: bbox.height.toInt(),
          isBGRA: Platform.isIOS,
          sensorOrientation: _cameraController!.description.sensorOrientation,
        );

        final result = await compute(_cropAndResizeFace, cropParams);

        if (result != null) {
          _processedFaceBytes = result;
          debugPrint(
            '[CameraScreen] âœ… Wajah berhasil di-crop & resize â†’ '
            '224Ã—224 (${result.length} bytes JPEG)',
          );
        }
      }

      // â”€â”€ 4. Process Hand Landmark Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      // Only collect when at least one hand was actually detected.
      // detectHands returns (left: null, right: null) on failure/no-hands,
      // so we must check the inner fields, not just the record itself.
      if (handResults != null && (handResults.left != null || handResults.right != null)) {
        try {
          // â”€â”€ Mirror landmarks (x â†’ 1âˆ’x) + swap leftâ†”right â”€â”€â”€â”€â”€â”€â”€â”€â”€
          // Training used cv2.flip(frame, 1) before MediaPipe Holistic,
          // so the model saw mirrored images.  To replicate:
          //   â€¢ Mirror each landmark's x-coordinate
          //   â€¢ What the plugin called "left" on the raw (non-mirrored)
          //     image becomes "right" in the mirrored image, & vice-versa
          final mirroredLeft = handResults.right?.mirrorX();
          final mirroredRight = handResults.left?.mirrorX();

          final features = _featureExtractor.extractFrameFeatures(
            leftHand: mirroredLeft,
            rightHand: mirroredRight,
          );
          _motionSequence.add(features);

          final count = _motionSequence.length;
          debugPrint(
            '[CameraScreen] ğŸ¤Ÿ Frame collected: '
            '$count/60',
          );

          // Mark ready once we hit the minimum threshold.
          if (!_isMotionDataReady && count >= _minFramesToSend) {
            _isMotionDataReady = true;
            debugPrint(
              '[CameraScreen] âœ… Motion data sendable! '
              '$count frames (min $_minFramesToSend).',
            );
            if (mounted) setState(() {});
          }

          // Update UI progress periodically
          if (count <= _targetFrames && count % 5 == 0 && mounted) {
            setState(() {});
          }
        } catch (e) {
          debugPrint('[CameraScreen] Hand landmark error: $e');
        }
      }
    } catch (e) {
      debugPrint('[CameraScreen] Error processing frame: $e');
    } finally {
      _isProcessingFrame = false;
    }
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  //  Build ML Kit InputImage from CameraImage
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  InputImage? _buildInputImage(CameraImage cameraImage) {
    if (_cameraController == null) return null;

    final camera = _cameraController!.description;
    final sensorOrientation = camera.sensorOrientation;

    InputImageRotation? rotation;
    if (Platform.isAndroid) {
      rotation = _intToRotation(sensorOrientation);
    } else if (Platform.isIOS) {
      rotation = InputImageRotation.rotation0deg;
    }
    if (rotation == null) return null;

    // ML Kit fromBytes() on Android only supports NV21 and YV12.
    // Camera is set to yuv420 (3 planes) for hand_landmarker, so we
    // must convert the 3 YUV420 planes â†’ packed NV21 bytes.
    final Uint8List bytes;
    final InputImageFormat format;

    if (Platform.isAndroid && cameraImage.planes.length >= 3) {
      bytes = _yuv420ToNv21(cameraImage);
      format = InputImageFormat.nv21;
    } else {
      bytes = _concatenatePlanes(cameraImage);
      format = InputImageFormat.bgra8888;
    }

    final metadata = InputImageMetadata(
      size: Size(cameraImage.width.toDouble(), cameraImage.height.toDouble()),
      rotation: rotation,
      format: format,
      bytesPerRow: cameraImage.planes[0].bytesPerRow,
    );

    return InputImage.fromBytes(bytes: bytes, metadata: metadata);
  }

  /// Converts YUV_420_888 (3 separate planes) to packed NV21 format.
  ///
  /// NV21 layout: [Y plane (w*h)] + [VU interleaved (w*h/2)]
  ///
  /// On CameraX, planes[1] (U) and planes[2] (V) share the same native
  /// buffer with pixelStride=2. plane[2].bytes starts at V and contains
  /// V,U,V,U... which is exactly the NV21 chroma layout.
  Uint8List _yuv420ToNv21(CameraImage image) {
    final w = image.width;
    final h = image.height;
    final yPlane = image.planes[0];
    final vPlane = image.planes[2]; // VU interleaved on CameraX = NV21
    final yRowStride = yPlane.bytesPerRow;
    final uvRowStride = vPlane.bytesPerRow;
    final chromaH = h ~/ 2;
    final nv21Size = w * h + w * chromaH;
    final nv21 = Uint8List(nv21Size);

    // â”€â”€ Copy Y plane â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if (yRowStride == w) {
      // Fast path: no row padding
      nv21.setRange(0, w * h, yPlane.bytes);
    } else {
      // Slow path: skip row padding
      for (int row = 0; row < h; row++) {
        final srcOff = row * yRowStride;
        final dstOff = row * w;
        nv21.setRange(dstOff, dstOff + w, yPlane.bytes, srcOff);
      }
    }

    // â”€â”€ Copy VU chroma â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final chromaOffset = w * h;
    if (uvRowStride == w) {
      // Fast path: chroma data is already tightly packed VU
      final chromaLen = min(w * chromaH, vPlane.bytes.length);
      nv21.setRange(chromaOffset, chromaOffset + chromaLen, vPlane.bytes);
    } else {
      // Slow path: skip row padding
      for (int row = 0; row < chromaH; row++) {
        final srcOff = row * uvRowStride;
        final dstOff = chromaOffset + row * w;
        final copyLen = min(w, vPlane.bytes.length - srcOff);
        if (copyLen <= 0) break;
        nv21.setRange(dstOff, dstOff + copyLen, vPlane.bytes, srcOff);
      }
    }

    return nv21;
  }

  Uint8List _concatenatePlanes(CameraImage image) {
    final WriteBuffer buffer = WriteBuffer();
    for (final plane in image.planes) {
      buffer.putUint8List(plane.bytes);
    }
    return buffer.done().buffer.asUint8List();
  }

  InputImageRotation _intToRotation(int sensorOrientation) {
    switch (sensorOrientation) {
      case 0:
        return InputImageRotation.rotation0deg;
      case 90:
        return InputImageRotation.rotation90deg;
      case 180:
        return InputImageRotation.rotation180deg;
      case 270:
        return InputImageRotation.rotation270deg;
      default:
        return InputImageRotation.rotation0deg;
    }
  }

  /// Resets the motion sequence buffer so the user can re-collect.
  void _resetMotionSequence() {
    _motionSequence.clear();
    _isMotionDataReady = false;
    if (mounted) setState(() {});
    debugPrint('[CameraScreen] ğŸ”„ Motion sequence reset.');
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  //  Send Button Handler
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  Future<void> _onSendPressed() async {
    if (_isSending) return;
    setState(() => _isSending = true);

    try {
      // Stop image stream before navigating
      await _cameraController?.stopImageStream();

      if (_processedFaceBytes != null) {
        final dir = await getTemporaryDirectory();
        final filePath = '${dir.path}/face_${DateTime.now().millisecondsSinceEpoch}.jpg';
        _processedFaceFile = File(filePath);
        await _processedFaceFile!.writeAsBytes(_processedFaceBytes!);

        debugPrint('[CameraScreen] ğŸ“ Processed face saved: $filePath');
      } else {
        debugPrint('[CameraScreen] âš ï¸ Tidak ada wajah terdeteksi untuk dikirim.');
      }

      // â”€â”€ Interpolate motion sequence to model's expected 60 frames â”€â”€
      List<List<double>>? interpolatedSequence;
      if (_motionSequence.isNotEmpty) {
        interpolatedSequence = BisindoFeatureExtractor.interpolateSequence(_motionSequence);
        debugPrint(
          '[CameraScreen] ğŸ“¦ Motion sequence interpolated: '
          '${_motionSequence.length} â†’ ${interpolatedSequence.length} frames '
          'Ã— ${_featureExtractor.totalFeatures} features',
        );
      } else {
        debugPrint('[CameraScreen] âš ï¸ No motion frames collected.');
      }

      // â”€â”€ Navigate ke Loading Screen, bawa data mentah â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      // Loading Screen akan memanggil cloud API selama animasi loading.
      if (mounted) {
        context.push(
          AppRoutes.emotionDetection,
          extra: <String, dynamic>{
            'faceImageBytes': _processedFaceBytes, // Uint8List? JPEG 224Ã—224
            'motionSequence': interpolatedSequence, // List<List<double>>? (60Ã—154)
          },
        );
      }
    } catch (e) {
      debugPrint('[CameraScreen] Error during send: $e');
    } finally {
      if (mounted) setState(() => _isSending = false);
    }
  }

  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  //  Build UI
  // â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      backgroundColor: const Color(0xFFFFF8E7),
      appBar: AppBar(
        backgroundColor: const Color(0xFFFFF8E7),
        elevation: 0,
        leading: IconButton(
          icon: const Icon(Icons.arrow_back, color: AppColors.textPrimary),
          onPressed: () {
            _cameraController?.stopImageStream().catchError((_) {});
            if (context.canPop()) {
              context.pop();
            } else {
              context.go(AppRoutes.home);
            }
          },
        ),
        title: const Text(
          'Deteksi Emosional',
          style: TextStyle(
            fontFamily: AppFonts.sfProRounded,
            fontSize: 18,
            fontWeight: FontWeight.w700,
            color: AppColors.textPrimary,
          ),
        ),
        centerTitle: true,
      ),
      body: LayoutBuilder(
        builder: (context, constraints) {
          final screenWidth = constraints.maxWidth;

          final circleSize = screenWidth * 0.55;
          final circleLeft = -(circleSize * 0.25);
          final circleBottom = -(circleSize * 0.4);

          final cimoSize = screenWidth * 0.4;
          const cimoLeft = 1.0;
          const cimoBottom = 10.0;

          return Stack(
            children: [
              // â”€â”€ Camera Preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              // â”€â”€ Camera Preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Positioned.fill(
                child: _isCameraInitialized && _cameraController != null
                    ? OverflowBox(
                        alignment: Alignment.center,
                        child: FittedBox(
                          fit: BoxFit.cover,
                          child: SizedBox(
                            width: _cameraController!.value.previewSize?.height ?? 1,
                            height: _cameraController!.value.previewSize?.width ?? 1,
                            child: Stack(
                              fit: StackFit.expand,
                              children: [
                                CameraPreview(_cameraController!),
                                if (_faceDetected && _faceBoundingBox != null)
                                  CustomPaint(
                                    painter: _FaceBoundingBoxPainter(
                                      boundingBox: _faceBoundingBox!,
                                      imageWidth: _imageWidth,
                                      imageHeight: _imageHeight,
                                      sensorOrientation: _sensorOrientation,
                                    ),
                                  ),
                              ],
                            ),
                          ),
                        ),
                      )
                    : Container(
                        color: Colors.black,
                        child: const Center(
                          child: CircularProgressIndicator(color: Color(0xFF41B37E)),
                        ),
                      ),
              ),

              // â”€â”€ Face status indicator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Positioned(
                top: 16,
                left: 0,
                right: 0,
                child: Center(
                  child: Container(
                    padding: const EdgeInsets.symmetric(horizontal: 16, vertical: 8),
                    decoration: BoxDecoration(
                      color: _faceDetected
                          ? const Color(0xFF41B37E).withValues(alpha: 0.85)
                          : Colors.red.withValues(alpha: 0.75),
                      borderRadius: BorderRadius.circular(20),
                    ),
                    child: Row(
                      mainAxisSize: MainAxisSize.min,
                      children: [
                        Icon(
                          _faceDetected ? Icons.face_retouching_natural : Icons.face_retouching_off,
                          color: Colors.white,
                          size: 18,
                        ),
                        const SizedBox(width: 6),
                        Text(
                          _faceDetected ? 'Wajah Terdeteksi' : 'Wajah Tidak Terdeteksi',
                          style: const TextStyle(
                            fontFamily: AppFonts.sfProRounded,
                            fontSize: 13,
                            fontWeight: FontWeight.w600,
                            color: Colors.white,
                          ),
                        ),
                      ],
                    ),
                  ),
                ),
              ),

              // â”€â”€ Motion sequence collection indicator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Positioned(
                top: 56,
                left: 0,
                right: 0,
                child: Center(
                  child: Container(
                    padding: const EdgeInsets.symmetric(horizontal: 14, vertical: 8),
                    decoration: BoxDecoration(
                      color: Colors.black.withValues(alpha: 0.6),
                      borderRadius: BorderRadius.circular(20),
                    ),
                    child: Column(
                      mainAxisSize: MainAxisSize.min,
                      children: [
                        Text(
                          _isMotionDataReady
                              ? 'âœ… Isyarat Siap!'
                              : _motionSequence.isEmpty
                              ? 'Tunjuk isyarat tanganmu'
                              : 'Rekam isyarat...',
                          style: const TextStyle(
                            fontFamily: AppFonts.sfProRounded,
                            fontSize: 12,
                            fontWeight: FontWeight.w600,
                            color: Colors.white,
                          ),
                        ),
                        const SizedBox(height: 6),
                        SizedBox(
                          width: 120,
                          height: 4,
                          child: ClipRRect(
                            borderRadius: BorderRadius.circular(2),
                            child: LinearProgressIndicator(
                              value: (_motionSequence.length / _targetFrames).clamp(0.0, 1.0),
                              backgroundColor: Colors.white.withValues(alpha: 0.25),
                              valueColor: AlwaysStoppedAnimation<Color>(
                                _isMotionDataReady
                                    ? const Color(0xFF41B37E)
                                    : const Color(0xFFFFC107),
                              ),
                            ),
                          ),
                        ),
                      ],
                    ),
                  ),
                ),
              ),

              // â”€â”€ Yellow circle background â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Positioned(
                left: circleLeft,
                bottom: circleBottom,
                child: Container(
                  width: circleSize,
                  height: circleSize,
                  decoration: const BoxDecoration(shape: BoxShape.circle, color: Color(0xFFFFBD30)),
                ),
              ),

              // â”€â”€ Cimo mascot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Positioned(
                left: cimoLeft,
                bottom: cimoBottom,
                child: SizedBox(
                  width: cimoSize,
                  height: cimoSize,
                  child: Image.asset(AssetPaths.cimoJoy, fit: BoxFit.contain),
                ),
              ),

              // â”€â”€ Send Button (bottom right) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              Positioned(
                right: 24,
                bottom: 40,
                child: GestureDetector(
                  onTap: _isSending ? null : _onSendPressed,
                  child: Container(
                    padding: const EdgeInsets.symmetric(horizontal: 24, vertical: 14),
                    decoration: BoxDecoration(
                      color: _isSending
                          ? Colors.grey
                          : (_faceDetected ? const Color(0xFF41B37E) : Colors.grey.shade400),
                      borderRadius: BorderRadius.circular(30),
                      boxShadow: [
                        BoxShadow(
                          color: _isSending
                              ? Colors.grey.shade600
                              : (_faceDetected ? const Color(0xFF2D7D58) : Colors.grey.shade600),
                          offset: const Offset(0, 4),
                          blurRadius: 0,
                        ),
                      ],
                    ),
                    child: Row(
                      mainAxisSize: MainAxisSize.min,
                      children: [
                        if (_isSending)
                          const SizedBox(
                            width: 20,
                            height: 20,
                            child: CircularProgressIndicator(color: Colors.white, strokeWidth: 2),
                          )
                        else ...[
                          Text(
                            'Kirim',
                            style: TextStyle(
                              fontFamily: AppFonts.sfProRounded,
                              fontSize: 18,
                              fontWeight: FontWeight.w700,
                              color: _faceDetected ? Colors.black : Colors.white,
                            ),
                          ),
                          const SizedBox(width: 8),
                          Icon(
                            Icons.send,
                            color: _faceDetected ? Colors.black : Colors.white,
                            size: 20,
                          ),
                        ],
                      ],
                    ),
                  ),
                ),
              ),
            ],
          );
        },
      ),
    );
  }
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// Face Bounding Box Overlay Painter
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class _FaceBoundingBoxPainter extends CustomPainter {
  final Rect boundingBox;
  final int imageWidth;
  final int imageHeight;
  final int sensorOrientation;

  _FaceBoundingBoxPainter({
    required this.boundingBox,
    required this.imageWidth,
    required this.imageHeight,
    required this.sensorOrientation,
  });

  /// Translate x from ML Kit coordinates â†’ canvas coordinates.
  /// Based on the official Google ML Kit Flutter example.
  double _translateX(double x, Size canvasSize) {
    switch (sensorOrientation) {
      case 90:
        return x * canvasSize.width / imageHeight;
      case 270:
        // 270Â° includes horizontal mirroring (front camera)
        return canvasSize.width - x * canvasSize.width / imageHeight;
      default:
        return x * canvasSize.width / imageWidth;
    }
  }

  /// Translate y from ML Kit coordinates â†’ canvas coordinates.
  double _translateY(double y, Size canvasSize) {
    switch (sensorOrientation) {
      case 90:
      case 270:
        return y * canvasSize.height / imageWidth;
      default:
        return y * canvasSize.height / imageHeight;
    }
  }

  @override
  void paint(Canvas canvas, Size size) {
    if (imageWidth == 0 || imageHeight == 0) return;

    final paint = Paint()
      ..color = const Color(0xFF41B37E)
      ..style = PaintingStyle.stroke
      ..strokeWidth = 2.5;

    // Translate all four edges using the ML Kit coordinate translator
    final x0 = _translateX(boundingBox.left, size);
    final x1 = _translateX(boundingBox.right, size);
    final y0 = _translateY(boundingBox.top, size);
    final y1 = _translateY(boundingBox.bottom, size);

    // For 270Â° rotation, x0 > x1 due to mirroring, so use min/max
    final scaledRect = Rect.fromLTRB(min(x0, x1), min(y0, y1), max(x0, x1), max(y0, y1));

    // Rounded rectangle
    canvas.drawRRect(RRect.fromRectAndRadius(scaledRect, const Radius.circular(8)), paint);

    // Corner accents
    final cornerLen = min(scaledRect.width, scaledRect.height) * 0.2;
    final cp = Paint()
      ..color = const Color(0xFF41B37E)
      ..style = PaintingStyle.stroke
      ..strokeWidth = 4
      ..strokeCap = StrokeCap.round;

    // Top-left
    canvas.drawLine(scaledRect.topLeft, Offset(scaledRect.left + cornerLen, scaledRect.top), cp);
    canvas.drawLine(scaledRect.topLeft, Offset(scaledRect.left, scaledRect.top + cornerLen), cp);
    // Top-right
    canvas.drawLine(scaledRect.topRight, Offset(scaledRect.right - cornerLen, scaledRect.top), cp);
    canvas.drawLine(scaledRect.topRight, Offset(scaledRect.right, scaledRect.top + cornerLen), cp);
    // Bottom-left
    canvas.drawLine(
      scaledRect.bottomLeft,
      Offset(scaledRect.left + cornerLen, scaledRect.bottom),
      cp,
    );
    canvas.drawLine(
      scaledRect.bottomLeft,
      Offset(scaledRect.left, scaledRect.bottom - cornerLen),
      cp,
    );
    // Bottom-right
    canvas.drawLine(
      scaledRect.bottomRight,
      Offset(scaledRect.right - cornerLen, scaledRect.bottom),
      cp,
    );
    canvas.drawLine(
      scaledRect.bottomRight,
      Offset(scaledRect.right, scaledRect.bottom - cornerLen),
      cp,
    );
  }

  @override
  bool shouldRepaint(_FaceBoundingBoxPainter oldDelegate) => oldDelegate.boundingBox != boundingBox;
}
